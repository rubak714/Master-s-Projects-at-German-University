{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bdaa1f",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c621fd",
   "metadata": {},
   "source": [
    "# 13th exercise: <font color=\"#C70039\">First Reinforcement Learning Q-Table learning</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Rubaiya Kabir Pranti</a>\n",
    "* Matriculation Number: <a href=\"https://www.gernotheisenberg.de/\"> 11146364</a>\n",
    "* Date:   16.01.2024\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. play with all hypterparameters including the actions and states table.\n",
    "5. add and implement an Ïµ-greedy strategy \n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b5efe-2b35-4935-af30-8c270bd3b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d567ab5-06ac-418f-96ca-892695bf9f61",
   "metadata": {},
   "source": [
    "### Create the possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d53fc92-527b-46c6-bc57-4e68423aa61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one dictionary location_to_state-> a physical location  'L1', 'L2'etc. maps to a numerical state \n",
    "# that the agent can understand and process\n",
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8\n",
    "}\n",
    "\n",
    "state_to_location = dict((state,location) for location, state in location_to_state.items()) #states\n",
    "#reversing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3fd99",
   "metadata": {},
   "source": [
    "### Create the actions & rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349627df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actions = [0,1,2,3,4,5,6,7,8] #possible actions the agent can take\n",
    "#actions = [0,1,2,3] #possible actions the agent can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b897aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewards = np.array([[0,1,0,0],\n",
    "                   #[1,0,1,0],\n",
    "                   #[0,1,0,0],\n",
    "                   #[0,0,0,0],\n",
    "                   #[0,1,0,0],\n",
    "                   #[0,0,1,0],\n",
    "                   #[0,0,0,1],\n",
    "                   #[0,0,0,0],\n",
    "                   #[0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80287845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                   #[1,0,1,0,1,0,0,0,0],\n",
    "                   #[0,1,0,0,0,1,0,0,0],\n",
    "                   #[0,0,0,0,0,0,1,0,0],\n",
    "                   #[0,1,0,0,0,0,0,1,0],\n",
    "                   #[0,0,1,0,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac22125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                   [1,0,1,0,1,0,0,0,0],\n",
    "                   [0,1,0,0,0,1,0,0,0],\n",
    "                   [0,0,0,0,0,0,1,0,0],\n",
    "                   [0,1,0,0,0,0,0,1,0],\n",
    "                   [0,0,1,0,0,0,0,0,0],\n",
    "                   [0,0,0,1,0,0,0,1,0],\n",
    "                   [0,0,0,0,1,0,1,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0]]) #represents the rewards tht the agent gets for moving from one state to another\n",
    "# it encourages the agent to find best path possible with highest rewards from a starting location to an end location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828a644-6339-4f56-8066-9dd3d629190a",
   "metadata": {},
   "source": [
    "### Def remaining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f112bf-e2e1-42a1-a9b3-df514af058c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the parameters\n",
    "gamma = 0.75 # discount factor\n",
    "alpha = 0.9  # learning rate\n",
    "# discount factor and learning rate tells how the future rewards are discounted \n",
    "# and how much new information overrides old information\n",
    "epsilon = 1.0  # starting value for epsilon\n",
    "min_epsilon = 0.01  # minimum value for epsilon\n",
    "decay_rate = 0.001  # decay rate for epsilon per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37973f00-e730-4fd2-b660-0ff268984e61",
   "metadata": {},
   "source": [
    "### Define agent and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a8c7ba-023b-414c-b007-64e789dcd9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class QAgent is defined with methods to perform the training\n",
    "# and to find the optimal route (get_optimal_route) based on the Q-table it learns\n",
    "\n",
    "class QAgent1():\n",
    "    # initialize everything\n",
    "    def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        # remember, the Q-value table is of size all actions x all states\n",
    "        M = len(location_to_state)\n",
    "        self.Q = np.zeros((M,M), dtype = None, order = 'C')\n",
    "        \n",
    "    # now, implement the training method for the agent\n",
    "    def training(self, start_location, end_location, iterations):\n",
    "\n",
    "        rewards_new = np.copy(self.rewards)\n",
    "\n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        \n",
    "        rewards_new[ending_state, ending_state] = 999\n",
    "        \n",
    "        # DEBUG\n",
    "        print(rewards_new)\n",
    "\n",
    "        # pick random current state\n",
    "        # iterations = the # of training cycles\n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0,9)\n",
    "            playable_actions = []\n",
    "\n",
    "            # iterate thru the rewards matrix to get states\n",
    "            # that are really reachable from the randomly chosen\n",
    "            # state and assign only those in a list since they are really playable\n",
    "            for j in range(9):\n",
    "                if rewards_new[current_state, j] > 0:\n",
    "                    playable_actions.append(j)\n",
    "\n",
    "            # choosing next random state\n",
    "            # however, make sure that playable_actions is not empty\n",
    "            if len(playable_actions) != 0:\n",
    "                # Randomly select a value from playable_actions\n",
    "                next_state = np.random.choice(playable_actions)\n",
    "    \n",
    "            # finding the difference in Q, often referred to as temporal difference\n",
    "            # by means of the Bellman's equation\n",
    "            TD = rewards_new[current_state, next_state] + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state, next_state]\n",
    "            # combine with the learning rate\n",
    "            self.Q[current_state, next_state] += self.alpha*TD\n",
    "            # DEBUG\n",
    "            #print(f\"Q[{current_state}, {next_state}]:\", self.Q[current_state, next_state])\n",
    "\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "\n",
    "        # print the optimal route from start to end\n",
    "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "\n",
    "    # compute the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        while(next_location != end_location):\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "        # DEBUG\n",
    "        print('Q-table:',Q)\n",
    "        print(\"optimal route:\", route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ece5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[999   1   0   0   0   0   0   0   0]\n",
      " [  1   0   1   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0]\n",
      " [  0   1   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0]\n",
      " [  0   0   0   0   1   0   1   0   1]\n",
      " [  0   0   0   0   0   0   0   1   0]]\n",
      "Q-table: [[3995.99804196 2249.4949161     0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [2997.99800052    0.         1688.12118684    0.         1688.12118579\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.         2249.49491647    0.            0.            0.\n",
      "  1267.09089047    0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.          951.31813021    0.            0.        ]\n",
      " [   0.         2249.49491647    0.            0.            0.\n",
      "     0.            0.         1267.09088187    0.        ]\n",
      " [   0.            0.         1688.12118735    0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.          714.48853577    0.\n",
      "     0.            0.         1267.09088134    0.        ]\n",
      " [   0.            0.            0.            0.         1688.12118109\n",
      "     0.          951.3174685     0.          951.30443518]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.         1267.09088582    0.        ]]\n",
      "optimal route: ['L9', 'L8', 'L5', 'L2', 'L1']\n"
     ]
    }
   ],
   "source": [
    "qagent1 = QAgent1(alpha, gamma, location_to_state, actions, rewards, state_to_location)\n",
    "# is creating an instance of a QAgent class with specific parameters \n",
    "qagent1.training('L9', 'L1', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e8d795",
   "metadata": {},
   "source": [
    "# with epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21222039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an epsilon parameter to the class and modify the training method\n",
    "\n",
    "class QAgent2():\n",
    "    # initialize everything, adding epsilon parameter\n",
    "    def __init__(self, alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon  #epsilon for the epsilon-greedy strategy\n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        M = len(location_to_state)\n",
    "        self.Q = np.zeros((M, M), dtype=float, order='C')\n",
    "        \n",
    "    def any_action_chosen(self, current_state, play_actions):\n",
    "        # implement epsilon-greedy strategy\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            # exploration>>>by chooseing a random action\n",
    "            next_state = np.random.choice(play_actions)\n",
    "        else:\n",
    "            # exploitation>>> choosing the best action from Q-table if available otherwise random action\n",
    "            current_q_values = self.Q[current_state, play_actions]\n",
    "            max_q_value = np.max(current_q_values)\n",
    "            if max_q_value == 0:\n",
    "                # If all Q-values are zero, choosing randomly among the playable actions==play_actions\n",
    "                next_state = np.random.choice(play_actions)\n",
    "            else:\n",
    "                # selecting the action with the highest Q-value\n",
    "                max_q_actions = np.where(current_q_values == max_q_value)[0]\n",
    "                next_state = np.random.choice(max_q_actions)\n",
    "        return next_state\n",
    "\n",
    "    # now, implementing the training method for the agent\n",
    "    def training(self, start_location, end_location, iterations, epsilon_decay):\n",
    "        rewards_new = np.copy(self.rewards)\n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        rewards_new[ending_state, ending_state] = 999\n",
    "\n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0, len(self.actions))\n",
    "            play_actions = [j for j in range(len(self.actions)) if rewards_new[current_state, j] > 0]\n",
    "\n",
    "            if play_actions:\n",
    "                next_state = self.any_action_chosen(current_state, play_actions)\n",
    "                # finding the difference in Q> often referred to as temporal difference\n",
    "                # by means of *******the Bellman's equation*********\n",
    "                TD = rewards_new[current_state, next_state] + self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state, next_state]\n",
    "                # combine with the learning rate\n",
    "                self.Q[current_state, next_state] += self.alpha * TD\n",
    "\n",
    "            # updating epsilon with decay\n",
    "            self.epsilon = max(self.epsilon - epsilon_decay, 0.01)  # ensuring epsilon does not go below 0.01\n",
    "\n",
    "        # after training, using the learned Q-table to determine the optimal path\n",
    "        # print the optimal route from start to end\n",
    "        return self.get_optimal_route(start_location, end_location)\n",
    "    \n",
    "    # compute the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location):\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "        while next_location != end_location:\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(self.Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "        return route\n",
    "        #print('Q-table:',Q)\n",
    "        #print(\"optimal route:\", route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bc2a1b-4291-4560-ba23-0dc145e01154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L9', 'L1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with  epsilon-greedy strategy\n",
    "qagent2 = QAgent2(alpha, gamma, epsilon, location_to_state, actions, rewards, state_to_location)\n",
    "# is creating an instance of a QAgent class with specific parameters \n",
    "qagent2.training('L9', 'L1', 1000, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8c93d",
   "metadata": {},
   "source": [
    "### <font color=\"FFC300\">Comments</font>:\n",
    "#### - Through epsilon-greedy strategy, now the optimal path is more straighforward because agent has already known through exploitation which will be the best action to get maximum cumulative reward \n",
    "#### - Even if i had increased the epsilon and decreased decay rate values for pushing for more exploration in the environment, the optimal path is same like before, directly from L9 to L1\n",
    "#### - if environment does not allow such path from L9 to L1 directly, it is illogical \n",
    "#### - but it would only be realistic if such a direct transition is possible and rewarding in our defined environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59eda45",
   "metadata": {},
   "source": [
    "### <font color=\"FFC300\">Comments</font>:\n",
    "#### -- In this context, the agent is supposed to learn the best actions to take in our given environment to maximize cumulative reward\n",
    "\n",
    "#### -- when the agent starts from state L9 and aims to reach the end state L1> the optimal path will depend on the Q-values which are a result of the given reward matrix and the exploration( the agent experinecs during training). If some paths have a reward of zero, the agent may still cross those paths during the exploration phase only, but it will learn over time to give priority to paths that yield higher cumulative rewards\n",
    "\n",
    "#### -- epsilon-greedy strategy is a common approach used in Q-learning to balance exploration and exploitation\n",
    "\n",
    "#### -- with above strategy, the agent will mostly choose the best-known action known as exploitation but occasionally can choose a random action>>exploration. A generl approach is to start with a higher epsilon==more exploration and gradually it will be decreased as the agent learns more about the environment\n",
    "\n",
    "#### -- zeros typically indicate that there is no value in taking that action from that state because it doesn't lead to an improvement in the long-term cumulative reward\n",
    "\n",
    "#### -- In the training method, the agent undergoes a number of iterations where it updates its Q-values based on the Bellman equation by using the alpha and gamma hyperparameters\n",
    "\n",
    "#### -- After training, the get_optimal_route method is used to determine the best path from a starting to an ending using the learned Q-values\n",
    "\n",
    "#### -- The discount factor quantifies how much importance is given to future rewards\n",
    "\n",
    "#### -- when gamma = 0.75, alpha = 0.9, then the optimal route is ['L9', 'L8', 'L5', 'L2', 'L1'] for qagent.training('L9', 'L1', 1000)\n",
    "#### -- when gamma = 0.9, alpha = 0.9(constant), q values are increased in some states than before as importance is given to future rewards\n",
    "#### -- when gamma = 0.2, alpha = 0.9(constant), q values are decreased in those states than before as importance is not given to future rewards\n",
    "#### -- when gamma = 0.75(constant), alpha = 0.1, q values are decreased in those states than before as the new information did not override those old information at a great extent\n",
    "#### -- when gamma = 0.75(constant), alpha = 0.5, q values are almost like the value achieved when alpha = 0.9\n",
    "#### -- if actions has been limitd to [0,1,2,3] only, the optimal path will be same like epsilon greedy strategy which is turned to optimal route: ['L9', 'L1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305f31a-215d-44aa-9625-19e18e03f12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
